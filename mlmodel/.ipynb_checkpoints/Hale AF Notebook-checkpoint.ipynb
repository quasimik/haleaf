{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hale AF\n",
    "Alright let's do this yo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The steps\n",
    "Getting your model ready for predictions can be done in 5 steps:\n",
    "1. Save your model to a file\n",
    "1. Upload the saved model to [Google Cloud Storage](https://cloud.google.com/storage)\n",
    "1. Create a model resource on ML Engine\n",
    "1. Create a model version (linking your scikit-learn model)\n",
    "1. Make an online prediction\n",
    "---\n",
    "## For HAF\n",
    "1. Build model using data csv and random tree classifier\n",
    "1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env variables yo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=haleaf-lahax18\n",
      "env: BUCKET_ID=haleaf-lahax18-bucket\n",
      "env: MODEL_NAME=haf-diabetes\n",
      "env: VERSION_NAME=v1\n",
      "env: REGION=us-central1\n"
     ]
    }
   ],
   "source": [
    "%env PROJECT_ID haleaf-lahax18\n",
    "%env BUCKET_ID haleaf-lahax18-bucket\n",
    "%env MODEL_NAME haf_diabetes\n",
    "%env VERSION_NAME v1\n",
    "%env REGION us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Train/Save the model \n",
    "First, the data is loaded into a pandas DataFrame that can be used by scikit-learn. Then a simple model is created and fit against the training data. Lastly, sklearn's built in version of joblib is used to save the model to a file that can be uploaded to ML Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8677109575030375\n",
      "Model trained and saved\n"
     ]
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# Define the format of your input data including unused columns (These are the columns from the census data files)\n",
    "CATEGORICAL_COLUMNS = [ # 24\n",
    "    'age_group',\n",
    "    'sex',\n",
    "    'married',\n",
    "    'education',\n",
    "    'employment',\n",
    "    'income',\n",
    "    'own_home',\n",
    "    'veteran',\n",
    "    'alchohol',\n",
    "    'smoke',\n",
    "    'race',\n",
    "    'state',\n",
    "    'high_blood_pressure',\n",
    "    'high_blood_cholestrol',\n",
    "    'heart_attack',\n",
    "    'angina',\n",
    "    'asthma',\n",
    "    'skin_cancer',\n",
    "    'other_cancer',\n",
    "    'Chronic_Obstructive_Pulmonary_Disease',\n",
    "    'arthritis',\n",
    "    'depression',\n",
    "    'kidney_disease',\n",
    "    'diabetes' ]\n",
    "\n",
    "# Categorical columns are columns that need to be turned into a numerical value to be used by scikit-learn\n",
    "COLUMNS = [ # 27-26\n",
    "    'index',\n",
    "    'age_group',\n",
    "    'height_cm',\n",
    "    'weight_kg',\n",
    "    'sex',\n",
    "    'married',\n",
    "    'education',\n",
    "    'employment',\n",
    "    'income',\n",
    "    'own_home',\n",
    "    'veteran',\n",
    "    'alchohol',\n",
    "    'smoke',\n",
    "    'race',\n",
    "    'state',\n",
    "    'high_blood_pressure',\n",
    "    'high_blood_cholestrol',\n",
    "    'heart_attack',\n",
    "    'angina',\n",
    "    'asthma',\n",
    "    'skin_cancer',\n",
    "    'other_cancer',\n",
    "    'Chronic_Obstructive_Pulmonary_Disease',\n",
    "    'arthritis',\n",
    "    'depression',\n",
    "    'kidney_disease',\n",
    "    'diabetes' ]\n",
    "\n",
    "# print(len(COLUMNS)) # 27\n",
    "# print(len(CATEGORICAL_COLUMNS)) # 24\n",
    "\n",
    "\n",
    "wtf = 'diabetes'\n",
    "\n",
    "\n",
    "# Load the training census dataset\n",
    "with open('./all_h&w_raw.csv', 'r') as train_data:\n",
    "    raw_training_data = pd.read_csv(train_data)\n",
    "\n",
    "# Remove the column we are trying to predict ('income-level') from our features list\n",
    "# Convert the Dataframe to a lists of lists\n",
    "# shittyfeatures = raw_training_data.as_matrix().tolist()\n",
    "# print(len(shittyfeatures[0]))\n",
    "\n",
    "features = raw_training_data.drop(columns='index').drop(columns=wtf).as_matrix().tolist()\n",
    "# print(len(features))\n",
    "# Create our training labels list, convert the Dataframe to a lists of lists\n",
    "labels = (raw_training_data[wtf] == 1).as_matrix().tolist()\n",
    "\n",
    "\n",
    "# Remove first index\n",
    "del COLUMNS[0] # 26\n",
    "\n",
    "\n",
    "# # Load the test census dataset\n",
    "# with open('./census_data/adult.test', 'r') as test_data:\n",
    "#     raw_testing_data = pd.read_csv(test_data, names=COLUMNS, skiprows=1)\n",
    "# # Remove the column we are trying to predict ('income-level') from our features list\n",
    "# # Convert the Dataframe to a lists of lists\n",
    "# test_features = raw_testing_data.drop('income-level', axis=1).as_matrix().tolist()\n",
    "# # Create our training labels list, convert the Dataframe to a lists of lists\n",
    "# test_labels = (raw_testing_data['income-level'] == ' >50K.').as_matrix().tolist()\n",
    "\n",
    "\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size=0.33, random_state=69)\n",
    "# # print(len(features))\n",
    "# # print(len(train_features)) # 670\n",
    "# # print(len(test_features))\n",
    "# print(len(train_features[0])) # 25\n",
    "# # print(len(train_features))\n",
    "# # Since the census data set has categorical features, we need to convert\n",
    "# # them to numerical values. We'll use a list of pipelines to convert each\n",
    "# # categorical column and then use FeatureUnion to combine them before calling\n",
    "# # the RandomForestClassifier.\n",
    "# categorical_pipelines = []\n",
    "\n",
    "# # Each categorical column needs to be extracted individually and converted to a numerical value.\n",
    "# # To do this, each categorical column will use a pipeline that extracts one feature column via\n",
    "# # SelectKBest(k=1) and a LabelBinarizer() to convert the categorical value to a numerical one.\n",
    "# # A scores array (created below) will select and extract the feature column. The scores array is\n",
    "# # created by iterating over the COLUMNS and checking if it is a CATEGORICAL_COLUMN.\n",
    "# for i, col in enumerate(COLUMNS):\n",
    "# #     if i == 20:\n",
    "# #         continue\n",
    "# #     print('i' + str(i))\n",
    "#     if col in CATEGORICAL_COLUMNS:\n",
    "#         # Create a scores array to get the individual categorical column.\n",
    "#         # Example:\n",
    "#         #  data = [39, 'State-gov', 77516, 'Bachelors', 13, 'Never-married', 'Adm-clerical', \n",
    "#         #         'Not-in-family', 'White', 'Male', 2174, 0, 40, 'United-States']\n",
    "#         #  scores = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "#         #\n",
    "#         # Returns: [['Sate-gov']]\n",
    "#         scores = []\n",
    "#         # Build the scores array\n",
    "#         for j in range(len(COLUMNS[:-1])):\n",
    "# #             if j == 0 or j == 20:\n",
    "# #                 continue\n",
    "#             if i == j: # This column is the categorical column we want to extract.\n",
    "#                 scores.append(1) # Set to 1 to select this column\n",
    "#             else: # Every other column should be ignored.\n",
    "#                 scores.append(0)\n",
    "#         skb = SelectKBest(k=1)\n",
    "#         skb.scores_ = scores\n",
    "# #         print(len(scores))\n",
    "# #         print(len(train_features[0]))\n",
    "#         # Convert the categorical column to a numerical value\n",
    "#         lbn = LabelBinarizer()\n",
    "#         r = skb.transform(train_features)\n",
    "#         lbn.fit(r)\n",
    "#         # Create the pipeline to extract the categorical feature\n",
    "#         categorical_pipelines.append(\n",
    "#             ('categorical-{}'.format(i), Pipeline([\n",
    "#                 ('SKB-{}'.format(i), skb),\n",
    "#                 ('LBN-{}'.format(i), lbn)])))\n",
    "# #     print(len(categorical_pipelines))\n",
    "# preprocess = FeatureUnion(categorical_pipelines)\n",
    "# print(len(preprocess.transformer_list))\n",
    "\n",
    "# # Create pipeline to extract the numerical features\n",
    "# skb = SelectKBest(k=2)\n",
    "# # From COLUMNS use the features that are numerical\n",
    "# skb.scores_ = [0,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0] # 26\n",
    "# print(len(COLUMNS))\n",
    "# # print(len(categorical_pipelines))\n",
    "# categorical_pipelines.append(('numerical', skb))\n",
    "# print('{}'.format(categorical_pipelines))\n",
    "# # print(len(categorical_pipelines))\n",
    "# # print(len(train_features[0]))\n",
    "\n",
    "# # Combine all the features using FeatureUnion\n",
    "# preprocess = FeatureUnion(categorical_pipelines)\n",
    "\n",
    "# print(len(preprocess.transformer_list))\n",
    "\n",
    "# # Create the classifier\n",
    "# classifier = RandomForestClassifier()\n",
    "\n",
    "# # Transform the features and fit them to the classifier\n",
    "# pt = preprocess.transform(train_features)\n",
    "# classifier.fit(pt, train_labels)\n",
    "\n",
    "# # Create the overall model as a single pipeline\n",
    "# pipeline = Pipeline([\n",
    "#     ('union', preprocess),\n",
    "#     ('classifier', classifier)\n",
    "# ])\n",
    "\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(train_features, train_labels)\n",
    "sc = classifier.score(test_features, test_labels)\n",
    "print(sc)\n",
    "\n",
    "# Export the model to a file\n",
    "joblib.dump(pipeline, 'model.joblib')\n",
    "\n",
    "print('Model trained and saved')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import googleapiclient.discovery\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the format of your input data including unused columns (These are the columns from the census data files)\n",
    "# COLUMNS = (\n",
    "#     'age',\n",
    "#     'workclass',\n",
    "#     'fnlwgt',\n",
    "#     'education',\n",
    "#     'education-num',\n",
    "#     'marital-status',\n",
    "#     'occupation',\n",
    "#     'relationship',\n",
    "#     'race',\n",
    "#     'sex',\n",
    "#     'capital-gain',\n",
    "#     'capital-loss',\n",
    "#     'hours-per-week',\n",
    "#     'native-country',\n",
    "#     'income-level'\n",
    "# )\n",
    "\n",
    "# Categorical columns are columns that need to be turned into a numerical value to be used by scikit-learn\n",
    "# CATEGORICAL_COLUMNS = (\n",
    "#     'workclass',\n",
    "#     'education',\n",
    "#     'marital-status',\n",
    "#     'occupation',\n",
    "#     'relationship',\n",
    "#     'race',\n",
    "#     'sex',\n",
    "#     'native-country'\n",
    "# )\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "with open('./1000sample2015.csv', 'r') as data:\n",
    "    raw_data = pd.read_csv(data)\n",
    "\n",
    "# Get cols\n",
    "wanted_cols = ('_AGEG5YR', 'HTM4', 'WTKG3', 'SEX', 'MARITAL', \n",
    "               'EDUCA', 'EMPLOY1', '_INCOMG', 'RENTHOM1', 'VETERAN3', \n",
    "               'DRNK3GE5', '_SMOKER3', '_MRACE1', '_STATE', 'BPHIGH4', \n",
    "               'TOLDHI2', 'CVDINFR4', 'CVDCRHD4', 'ASTHMA3', \n",
    "               'CHCSCNCR', 'CHCOCNCR', 'CHCCOPD1', 'HAVARTH3', \n",
    "               'ADDEPEV2', 'CHCKIDNY', 'DIABETE3')\n",
    "raw_data = raw_data.filter(items=wanted_cols)\n",
    "\n",
    "# Feature we want to predict\n",
    "wtp = 'SEX'\n",
    "\n",
    "# Remove the column we are trying to predict [20] ('other_cancer') from features list\n",
    "# Create features list\n",
    "# Convert the Dataframe to a lists of lists\n",
    "features = raw_data.drop(wtp, axis='columns').as_matrix().tolist()\n",
    "\n",
    "# Create labels list\n",
    "# Convert the Dataframe to a lists of lists\n",
    "# 2: no, 1: yes, 7,9: NaN\n",
    "labels = (raw_data[wtp]).as_matrix().tolist()\n",
    "\n",
    "# import pprint\n",
    "# pp = pprint.PrettyPrinter(indent=2)\n",
    "# pp.pprint(features)\n",
    "# pp.pprint(labels)\n",
    "\n",
    "# Split into train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "# # Since the census data set has categorical features, we need to convert\n",
    "# # them to numerical values. We'll use a list of pipelines to convert each\n",
    "# # categorical column and then use FeatureUnion to combine them before calling\n",
    "# # the RandomForestClassifier.\n",
    "# categorical_pipelines = []\n",
    "\n",
    "# # Each categorical column needs to be extracted individually and converted to a numerical value.\n",
    "# # To do this, each categorical column will use a pipeline that extracts one feature column via\n",
    "# # SelectKBest(k=1) and a LabelBinarizer() to convert the categorical value to a numerical one.\n",
    "# # A scores array (created below) will select and extract the feature column. The scores array is\n",
    "# # created by iterating over the COLUMNS and checking if it is a CATEGORICAL_COLUMN.\n",
    "# for i, col in enumerate(COLUMNS[:-1]):\n",
    "#     if col in CATEGORICAL_COLUMNS:\n",
    "#         # Create a scores array to get the individual categorical column.\n",
    "#         # Example:\n",
    "#         #  data = [39, 'State-gov', 77516, 'Bachelors', 13, 'Never-married', 'Adm-clerical', \n",
    "#         #         'Not-in-family', 'White', 'Male', 2174, 0, 40, 'United-States']\n",
    "#         #  scores = [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "#         #\n",
    "#         # Returns: [['Sate-gov']]\n",
    "#         scores = []\n",
    "#         # Build the scores array\n",
    "#         for j in range(len(COLUMNS[:-1])):\n",
    "#             if i == j: # This column is the categorical column we want to extract.\n",
    "#                 scores.append(1) # Set to 1 to select this column\n",
    "#             else: # Every other column should be ignored.\n",
    "#                 scores.append(0)\n",
    "#         skb = SelectKBest(k=1)\n",
    "#         skb.scores_ = scores\n",
    "#         # Convert the categorical column to a numerical value\n",
    "#         lbn = LabelBinarizer()\n",
    "#         r = skb.transform(train_features)\n",
    "#         lbn.fit(r)\n",
    "#         # Create the pipeline to extract the categorical feature\n",
    "#         categorical_pipelines.append(\n",
    "#             ('categorical-{}'.format(i), Pipeline([\n",
    "#                 ('SKB-{}'.format(i), skb),\n",
    "#                 ('LBN-{}'.format(i), lbn)])))\n",
    "\n",
    "# # Create pipeline to extract the numerical features\n",
    "# skb = SelectKBest(k=6)\n",
    "# # From COLUMNS use the features that are numerical\n",
    "# skb.scores_ = [1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0]\n",
    "# categorical_pipelines.append(('numerical', skb))\n",
    "\n",
    "# # Combine all the features using FeatureUnion\n",
    "# preprocess = FeatureUnion(categorical_pipelines)\n",
    "\n",
    "# # Create the classifier\n",
    "# classifier = RandomForestClassifier()\n",
    "\n",
    "# # Transform the features and fit them to the classifier\n",
    "# classifier.fit(preprocess.transform(train_features), train_labels)\n",
    "\n",
    "# # Create the overall model as a single pipeline\n",
    "# pipeline = Pipeline([\n",
    "#     ('union', preprocess),\n",
    "#     ('classifier', classifier)\n",
    "# ])\n",
    "\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "# clf = KNeighborsClassifier(1)\n",
    "clf.fit(X_train, y_train)\n",
    "score = clf.score(X_test, y_test)\n",
    "print('Feature: ' + wtp)\n",
    "print('Accuracy: ' + str(score))\n",
    "proba = clf.predict_proba(X_test)\n",
    "# proba_label = np.insert(proba, 0, y_test, axis=1)\n",
    "for i in np.random.randint(0, len(y_test), 6):\n",
    "    print(str(y_test[i]) + ' / ' + str(proba[i]))\n",
    "\n",
    "# Export the model to a file\n",
    "joblib.dump(clf, 'model.joblib')\n",
    "\n",
    "print('Model trained and saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Upload the model\n",
    "Next, you'll need to upload the model to your project's storage bucket in GCS. To use your model with ML Engine, it needs to be uploaded to Google Cloud Storage (GCS). This step takes your local ‘model.joblib’ file and uploads it GCS via the Cloud SDK using gsutil.\n",
    "\n",
    "Before continuing, make sure you're [properly authenticated](https://cloud.google.com/sdk/gcloud/reference/auth/) and have [access to the bucket](https://cloud.google.com/storage/docs/access-control/). This next command sets your project to the one specified above.\n",
    "\n",
    "Note: If you get an error below, make sure the Cloud SDK is installed in the kernel's environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The exact file name of of the exported model you upload to GCS is important! Your model must be named  “model.joblib”, “model.pkl”, or “model.bst” with respect to the library you used to export it. This restriction ensures that the model will be safely reconstructed later by using the same technique for import as was used during export."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://./model.joblib [Content-Type=application/octet-stream]...\n",
      "/ [1 files][  7.7 MiB/  7.7 MiB]                                                \n",
      "Operation completed over 1 objects/7.7 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp ./model.joblib gs://$BUCKET_ID/model.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Create a model resource\n",
    "Cloud ML Engine organizes your trained models using model and version resources. A Cloud ML Engine model is a container for the versions of your machine learning model. For more information on model resources and model versions look [here](https://cloud.google.com/ml-engine/docs/deploying-models#creating_a_model_version). \n",
    "\n",
    "At this step, you create a container that you can use to hold several different versions of your actual model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ml engine model [projects/haleaf-lahax18/models/haf_diabetes].\r\n"
     ]
    }
   ],
   "source": [
    "! MODEL_NAME=haf_diabetes\n",
    "! gcloud ml-engine models create haf_diabetes --regions us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Create a model version\n",
    "\n",
    "Now it’s time to get your model online and ready for predictions. The model version requires a few components as specified [here](https://cloud.google.com/ml-engine/reference/rest/v1/projects.models.versions#Version).\n",
    "\n",
    "* __name__ - The name specified for the version when it was created. This will be the `VERSION_NAME` variable you declared at the beginning.\n",
    "* __deployment Uri__ (curl) or __origin__ (gcloud) - The Google Cloud Storage location of the trained model used to create the version. This is the bucket that you uploaded the model to with your `BUCKET_ID`\n",
    "* __runtime__ version - The Google Cloud ML runtime version to use for this deployment. This is set to 1.4\n",
    "* __framework__ - The framework specifies if you are using: `TENSORFLOW`, `SCIKIT_LEARN`, `XGBOOST`. This is set to `SCIKIT_LEARN`\n",
    "* __pythonVersion__ - This specifies whether you’re using Python 2.7 or Python 3.5. The default value is set to `“2.7”`, if you are using Python 3.5, set the value to `“3.5”`\n",
    "\n",
    "\n",
    "Note: It can take several minutes for you model to be available.\n",
    "\n",
    "Note: If you require a feature of scikit-learn that isn’t available in the publicly released version yet, you can specify “runtimeVersion”: “HEAD” instead, and that would get the latest version of scikit-learn available from the github repo. Otherwise the following versions will be used:\n",
    "* scikit-learn: 0.19.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"name\": \"projects/haleaf-lahax18/operations/create_haf_diabetes_v1-1522614770624\",\r\n",
      "  \"metadata\": {\r\n",
      "    \"@type\": \"type.googleapis.com/google.cloud.ml.v1.OperationMetadata\",\r\n",
      "    \"createTime\": \"2018-04-01T20:32:51Z\",\r\n",
      "    \"operationType\": \"CREATE_VERSION\",\r\n",
      "    \"modelName\": \"projects/haleaf-lahax18/models/haf_diabetes\",\r\n",
      "    \"version\": {\r\n",
      "      \"name\": \"projects/haleaf-lahax18/models/haf_diabetes/versions/v1\",\r\n",
      "      \"deploymentUri\": \"gs://haleaf-lahax18-bucket/\",\r\n",
      "      \"createTime\": \"2018-04-01T20:32:50Z\",\r\n",
      "      \"runtimeVersion\": \"1.4\",\r\n",
      "      \"framework\": \"SCIKIT_LEARN\",\r\n",
      "      \"pythonVersion\": \"3.5\"\r\n",
      "    }\r\n",
      "  }\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "! curl -X POST -H \"Content-Type: application/json\" \\\n",
    "   -d '{\"name\": \"'$VERSION_NAME'\", \"deploymentUri\": \"gs://'$BUCKET_ID'/\", \"runtimeVersion\": \"1.4\", \"framework\": \"SCIKIT_LEARN\", \"pythonVersion\": \"3.5\"}' \\\n",
    "   -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    https://ml.googleapis.com/v1/projects/$PROJECT_ID/models/haf_diabetes/versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Make an online prediction\n",
    "It’s time to make an online prediction with your newly deployed model. Before you begin, you'll need to take some of the test data and prepare it, so that the test data can be used by the deployed model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Show a person that makes <=50K:\n",
      "\tFeatures: [25, ' Private', 226802, ' 11th', 7, ' Never-married', ' Machine-op-inspct', ' Own-child', ' Black', ' Male', 0, 0, 40, ' United-States'] --> Label: False\n",
      "\n",
      "Show a person that makes >50K:\n",
      "\tFeatures: [44, ' Private', 160323, ' Some-college', 10, ' Married-civ-spouse', ' Machine-op-inspct', ' Husband', ' Black', ' Male', 7688, 0, 40, ' United-States'] --> Label: True\n"
     ]
    }
   ],
   "source": [
    "# Get one person that makes <=50K and one that makes >50K to test our model.\n",
    "print('Show a person that makes <=50K:')\n",
    "print('\\tFeatures: {0} --> Label: {1}\\n'.format(test_features[0], test_labels[0]))\n",
    "\n",
    "with open('less_than_50K.json', 'w') as outfile:\n",
    "  json.dump(test_features[0], outfile)\n",
    "\n",
    "  \n",
    "print('Show a person that makes >50K:')\n",
    "print('\\tFeatures: {0} --> Label: {1}'.format(test_features[3], test_labels[3]))\n",
    "\n",
    "with open('more_than_50K.json', 'w') as outfile:\n",
    "  json.dump(test_features[3], outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use gcloud to make online predictions\n",
    "Use the two people (as seen in the table) gathered in the previous step for the gcloud predictions.\n",
    "\n",
    "| **Person** | age | workclass | fnlwgt | education | education-num | marital-status | occupation |\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:\n",
    "| **1** | 25| Private | 226802 | 11th | 7 | Never-married | Machine-op-inspect |\n",
    "| **2** | 44| Private | 160323 | Some-college | 10 | Married-civ-spouse | Machine-op-inspct |\n",
    "\n",
    "| **Person** | relationship | race | sex | capital-gain | capital-loss | hours-per-week | native-country || (Label) income-level|\n",
    "|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:||:-:\n",
    "| **1** | Own-child | Black | Male | 0 | 0 | 40 | United-States || False (<=50K) |\n",
    "| **2** | Huasband | Black | Male | 7688 | 0 | 40 | United-States || True (>50K) |\n",
    "\n",
    "\n",
    "Creating a model version can take several minutes, check the status of your model version to see if it is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME  DEPLOYMENT_URI               STATE\r\n",
      "v1    gs://haleaf-lahax18-bucket/  READY\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud ml-engine versions list --model $MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME  DEPLOYMENT_URI               STATE\r\n",
      "v1    gs://haleaf-lahax18-bucket/  READY\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud ml-engine versions list --model $MODEL_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model with an online prediction using the data of a person who makes <=50K.\n",
    "\n",
    "Note: If you see an error, the model from Part 4 may not be created yet as it takes several minutes for a new model version to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud ml-engine predict --model $MODEL_NAME --version $VERSION_NAME --json-instances less_than_50K.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the model with an online prediction using the data of a person who makes >50K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True]\r\n"
     ]
    }
   ],
   "source": [
    "! gcloud ml-engine predict --model $MODEL_NAME --version $VERSION_NAME --json-instances more_than_50K.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Python to make online predictions\n",
    "Test the model with the entire test set and print out some of the results.\n",
    "\n",
    "Note: If running notebook server on Compute Engine, make sure to [\"allow full access to all Cloud APIs\".](https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#changeserviceaccountandscopes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set application credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"./gcp-credentials.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpError",
     "evalue": "<HttpError 400 when requesting https://ml.googleapis.com/v1/projects/haleaf-lahax18/models/haf-diabetes/versions/v1:predict?alt=json returned \"Request payload size exceeds the limit: 1572864 bytes.\">",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-353-c52dc374490c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     responses = service.projects().predict(\n\u001b[1;32m     16\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'instances'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     ).execute()\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/oauth2client/_helpers.py\u001b[0m in \u001b[0;36mpositional_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mpositional_parameters_enforcement\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPOSITIONAL_WARNING\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpositional_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/googleapiclient/http.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, http, num_retries)\u001b[0m\n\u001b[1;32m    839\u001b[0m       \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mHttpError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muri\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHttpError\u001b[0m: <HttpError 400 when requesting https://ml.googleapis.com/v1/projects/haleaf-lahax18/models/haf-diabetes/versions/v1:predict?alt=json returned \"Request payload size exceeds the limit: 1572864 bytes.\">"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "VERSION_NAME = os.environ['VERSION_NAME']\n",
    "MODEL_NAME = os.environ['MODEL_NAME']\n",
    "\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "name = 'projects/{}/models/{}'.format(PROJECT_ID, MODEL_NAME)\n",
    "name += '/versions/{}'.format(VERSION_NAME)\n",
    "\n",
    "# Due to the size of the data, it needs to be split in 2\n",
    "first_half = test_features[:int(len(test_features)/2)]\n",
    "second_half = test_features[int(len(test_features)/2):]\n",
    "\n",
    "complete_results = []\n",
    "for data in [first_half, second_half]:\n",
    "    responses = service.projects().predict(\n",
    "        name=name,\n",
    "        body={'instances': data}\n",
    "    ).execute()\n",
    "\n",
    "    if 'error' in responses:\n",
    "        print(response['error'])\n",
    "    else:\n",
    "        complete_results.extend(responses['predictions'])\n",
    "        \n",
    "# Print the first 10 responses\n",
    "for i, response in enumerate(complete_results[:10]):\n",
    "    print('Prediction: {}\\tLabel: {}'.format(response, test_labels[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Optional] Part 6: Verify Results\n",
    "Use a confusion matrix to create a visualization of the online predicted results from ML Engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>online</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>11587</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1651</td>\n",
       "      <td>2195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "online  False  True \n",
       "actual              \n",
       "False   11587    848\n",
       "True     1651   2195"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actual = pd.Series(test_labels, name='actual')\n",
    "online = pd.Series(complete_results, name='online')\n",
    "\n",
    "pd.crosstab(actual,online)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a confusion matrix create a visualization of the predicted results from the local model. These results should be identical to the results above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>local</th>\n",
       "      <th>False</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>11584</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>1696</td>\n",
       "      <td>2150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "local   False  True \n",
       "actual              \n",
       "False   11584    851\n",
       "True     1696   2150"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_results = pipeline.predict(test_features)\n",
    "local = pd.Series(local_results, name='local')\n",
    "\n",
    "pd.crosstab(actual,local)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Directly compare the two results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identical: 15145, different: 1136\n"
     ]
    }
   ],
   "source": [
    "identical = 0\n",
    "different = 0\n",
    "\n",
    "for i in range(len(complete_results)):\n",
    "    if complete_results[i] == local_results[i]:\n",
    "        identical += 1\n",
    "    else:\n",
    "        different += 1\n",
    "        \n",
    "print('identical: {}, different: {}'.format(identical,different))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all results are identical, it means you've successfully uploaded your local model to ML Engine and performed online predictions correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
